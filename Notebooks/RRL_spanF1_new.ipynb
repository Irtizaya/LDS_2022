{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia9-d5mmFu0d",
        "outputId": "7843e87f-7a02-4f33-cc76-eb0deb5d27bd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install allennlp\n",
        "!pip install --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4htMvz0ISEN",
        "outputId": "7f0309b7-4356-4608-d5e3-d054f07e7517"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# %cd /content/drive/My\\ Drive/Legal\\ DS/SCRF_RRL/rhetorical-role-baseline/\n",
        "# Ekstep corpus:\n",
        "# %cd /content/drive/My\\ Drive/Legal\\ DS/Paheli_new_corpus/semantic_segmentation/Corpus\n",
        "\n",
        "import pandas as pd\n",
        "from itertools import groupby\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5axwS-V8vrR",
        "outputId": "72bde9f1-b20c-42d8-e623-34bf14fd03bd"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My\\ Drive/Legal\\ DS/Paheli_new_corpus/semantic-segmentation/Corpus/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpFSw70fMpDU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "#import torchtext\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, KBinsDiscretizer\n",
        "import numpy as np\n",
        "# Pytorch Dataset\n",
        "class RRDataset(Dataset):\n",
        "  def __init__(self, path, tokenizer_path, label_to_ind, max_len):\n",
        "    self.encoding = []\n",
        "    self.labels = []\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    self.sep_token_id = self.tokenizer.sep_token_id\n",
        "    self.pad_token_id = self.tokenizer.pad_token_id\n",
        "    self.label_to_ind = label_to_ind\n",
        "    self.max_len = max_len\n",
        "\n",
        "    f = open(path)\n",
        "    data = json.load(f)\n",
        "    #ans?\n",
        "    ans = map(self.parse_doc,data)\n",
        "    text_labels = list(ans)\n",
        "    #labels, text, id\n",
        "    self.text = [x[1] for x in text_labels]\n",
        "    self.labels = [x[0] for x in text_labels]\n",
        "    self.id = [x[2] for x in text_labels]\n",
        "    self.text, self.labels, self.id = zip(*list(filter(lambda x: (len(x[0]) > 0 and len(x[1]) > 0), zip(self.text, self.labels,self.id))))\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return (self.text[item], self.labels[item], self.id[item])\n",
        "  \n",
        "  def parse_doc(self,x):\n",
        "    doc_labels, text = [],[]\n",
        "    id = x['id']\n",
        "    sentences = x['annotations'][0]['result']\n",
        "    for sentence in sentences:\n",
        "        #doc_labels.append(sentence['value']['labels'][0])\n",
        "        doc_labels.append(self.label_to_ind[sentence['value']['labels'][0]])\n",
        "        #text.append(sentence['value']['text'])\n",
        "        text.append(self.tokenizer.encode(sentence['value']['text'])[:self.max_len])\n",
        "    return (doc_labels, text, id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMLh8TqUkWVR"
      },
      "outputs": [],
      "source": [
        "from allennlp.data.dataset_readers.dataset_utils import enumerate_spans\n",
        "import torch\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx, sep_idx, max_width, label_to_ind):\n",
        "        self.pad_token_id = pad_idx\n",
        "        #sep token ???\n",
        "        self.sep_token_id = sep_idx\n",
        "        self.max_width = max_width\n",
        "        self.label_to_ind = label_to_ind\n",
        "\n",
        "    def pad_sentence_for_batch(self, tokens_lists, max_len: int):\n",
        "        pad_id = self.pad_token_id\n",
        "        toks_ids = []\n",
        "        att_masks = []\n",
        "        #pad each token in token list\n",
        "        #att mask = 1 * token len\n",
        "        for item_toks in tokens_lists:\n",
        "            padded_item_toks = item_toks + [pad_id] * (max_len - len(item_toks))\n",
        "            toks_ids.append(padded_item_toks)\n",
        "\n",
        "            att_mask = [1] * len(item_toks) + [0] * (max_len - len(item_toks))\n",
        "            att_masks.append(att_mask)\n",
        "            \n",
        "        return toks_ids, att_masks\n",
        "    \n",
        "    def pad_doc_for_batch(self, doc_lengths, labels, segment_ids, max_len):\n",
        "        lab_masks = []\n",
        "        sent_masks = []\n",
        "        seg_masks = []\n",
        "        for i  in range(len(labels)):\n",
        "          lab_item = labels[i] + [self.label_to_ind['MASK']]*(max_len - len(labels[i]))\n",
        "          lab_masks.append(lab_item)\n",
        "\n",
        "          seg_item = segment_ids[i] + [0]*(max_len - len(segment_ids[i]))\n",
        "          seg_masks.append(seg_item)\n",
        "\n",
        "          each_sent_mask = [1] * doc_lengths[i] + [0] * (max_len- doc_lengths[i])\n",
        "          sent_masks.append(each_sent_mask)\n",
        "\n",
        "        return sent_masks, lab_masks, seg_masks\n",
        "\n",
        "    def span_enumeration(self, sent_masks, max_width):\n",
        "        all_span_ids = []\n",
        "        for each in range(len(sent_masks)):\n",
        "            each_span_ids = enumerate_spans(sent_masks[each][(sent_masks[each].nonzero())], offset=0, max_span_width=max_width)\n",
        "            #each_span_ids = enumerate_spans(x[\"sentence_mask\"][each], offset=0, max_span_width=3)\n",
        "            all_span_ids.append(each_span_ids)\n",
        "\n",
        "        max_span_len = max([len(x) for x in all_span_ids])\n",
        "        span_ids = [x+[[0,0]]*(max_span_len-len(x)) for x in all_span_ids]\n",
        "        return span_ids\n",
        "    \n",
        "    \n",
        "    def seg_mask_fix(self,seg_inds):\n",
        "        max_path = self.max_width\n",
        "        counter = np.zeros((len(seg_inds)), dtype=np.int32)\n",
        "        seg_inds_fix = []\n",
        "        for b,  sent_inds in enumerate(seg_inds):\n",
        "            counter = 0\n",
        "            new_inds = []\n",
        "            for i , flag in enumerate(sent_inds):\n",
        "                path_flag = (counter >= max_path-1)\n",
        "                    \n",
        "                mask_step = flag | path_flag\n",
        "                new_inds.append(mask_step)\n",
        "                counter = counter + 1\n",
        "                counter = (1- mask_step)*counter*(counter < max_path)\n",
        "                \n",
        "            seg_inds_fix.append(new_inds)     \n",
        "        return seg_inds_fix\n",
        "\n",
        "    \n",
        "    def convert_labels_to_segments(self,labels):\n",
        "      seg_ids = []\n",
        "      for i  in range(len(labels)):\n",
        "          each_seg_id = []\n",
        "          prev = labels[i][0]\n",
        "          each_seg_id.append(0)\n",
        "          for j in range(1,len(labels[i])):\n",
        "              if(prev != labels[i][j]):\n",
        "                  each_seg_id[len(each_seg_id)-1] = 1\n",
        "                  each_seg_id.append(0)\n",
        "                  prev = labels[i][j]\n",
        "              else:\n",
        "                  each_seg_id.append(0)\n",
        "          each_seg_id[len(each_seg_id)-1] = 1\n",
        "          seg_ids.append(each_seg_id)\n",
        "      segments = self.seg_mask_fix(seg_ids)\n",
        "      return segments\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        docs , labels, id = list(zip(*batch))\n",
        "        doc_lengths = [len(x) for x in docs]\n",
        "        sent_lengths = []\n",
        "        for element in docs:\n",
        "          sent_lengths.append([len(i) for i in element])\n",
        "        \n",
        "        batch_sz = len(id)\n",
        "        batch_max_doc_length = max(doc_lengths)\n",
        "        batch_max_sent_length = max([max(sl) for sl in sent_lengths])\n",
        "\n",
        "        docs_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        att_mask = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "\n",
        "        segments = self.convert_labels_to_segments(labels)\n",
        "        padded_sent_mask, padded_label_id, padded_segments = self.pad_doc_for_batch(doc_lengths, labels, segments, batch_max_doc_length)\n",
        "\n",
        "        label_ids = torch.tensor(padded_label_id , dtype=torch.long)\n",
        "        segment_ids = torch.tensor(padded_segments , dtype=torch.long)\n",
        "        sent_mask = torch.tensor(padded_sent_mask, dtype=torch.long)\n",
        "\n",
        "        spans = torch.tensor(self.span_enumeration(sent_mask, self.max_width), dtype = torch.long)\n",
        "\n",
        "        for doc_idx, doc in enumerate(docs):\n",
        "            padded_token_lists, att_mask_lists = self.pad_sentence_for_batch(doc, batch_max_sent_length)\n",
        "\n",
        "            for sent_idx, (padded_tokens, att_masks) in enumerate(\n",
        "                    zip(padded_token_lists, att_mask_lists)):\n",
        "                docs_tensor[doc_idx, sent_idx, :] = torch.tensor(padded_tokens, dtype=torch.long)\n",
        "                att_mask[doc_idx, sent_idx, :] = torch.tensor(att_masks, dtype=torch.long)\n",
        "        \n",
        "\n",
        "        output = {\n",
        "            \"sentence_mask\": torch.tensor(sent_mask),\n",
        "            \"input_ids\": torch.tensor(docs_tensor),\n",
        "            \"attention_mask\": torch.tensor(att_mask),\n",
        "            \"label_ids\": torch.tensor(label_ids),\n",
        "            \"segment_mask\": torch.tensor(segment_ids),\n",
        "            \"span_indices\": torch.tensor(spans),\n",
        "            \"doc_name\": id\n",
        "        }\n",
        "        return output\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUq6uHx-QjFP",
        "outputId": "53380ed3-0855-48a9-9351-5550b949e5a1"
      },
      "outputs": [],
      "source": [
        "# LABELS = [\"MASK\",\"PREAMBLE\", \"NONE\", \"FAC\", \"ISSUE\", \"ARG_RESPONDENT\", \"ARG_PETITIONER\", \"ANALYSIS\", \"PRE_RELIED\",\n",
        "#               \"PRE_NOT_RELIED\", \"STA\", \"RLC\", \"RPC\", \"RATIO\"]\n",
        "\n",
        "# LABELS = [\"DEFAULT\", 'MASK', \"NONE\", \"Facts\", \"Argument\", \"Ratio of the decision\", \"Statute\", \"Precedent\", \"Ruling by Present Court\", \"Ruling by Lower Court\"]\n",
        "\n",
        "LABELS = ['MASK', 'OBJECTIVE', 'BACKGROUND', 'METHODS', 'RESULTS', 'CONCLUSIONS']\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Legal\\ DS\\ backup/Pubmed/\n",
        "\n",
        "\n",
        "labels_int = range(len(LABELS)) \n",
        "label_to_ind = dict( zip(LABELS,labels_int))\n",
        "#label_to_ind['MASK'] = len(label_to_ind)\n",
        "label_to_ind['START'] = len(label_to_ind)\n",
        "label_to_ind['STOP'] = len(label_to_ind)\n",
        "\n",
        "bert_model = \"bert-base-uncased\"\n",
        "#bert_model = \"zlucia/custom-legalbert\"\n",
        "\n",
        "train_dataset = RRDataset('pubmed_train_to_ekstep.json',tokenizer_path = bert_model, label_to_ind = label_to_ind, max_len = 128)\n",
        "dev_dataset = RRDataset('pubmed_dev_to_ekstep.json',tokenizer_path = bert_model, label_to_ind = label_to_ind, max_len = 128)\n",
        "#dev_dataset = RRDataset('dev.json',tokenizer_path = bert_model, label_to_ind = label_to_ind, max_len = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUqat4U-foU8",
        "outputId": "b41ffc64-4db9-4161-8b5f-bb96b88cfe85"
      },
      "outputs": [],
      "source": [
        "len(train_dataset), len(dev_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fujhATAWk_Wl"
      },
      "outputs": [],
      "source": [
        "#max_width = max span length\n",
        "#batch_size default 1\n",
        "\n",
        "train_dataloader =  DataLoader(train_dataset, batch_size=30, shuffle=True, collate_fn = MyCollate(pad_idx = train_dataset.pad_token_id, sep_idx = train_dataset.pad_token_id, max_width = 2, label_to_ind=label_to_ind))\n",
        "\n",
        "dev_dataloader =  DataLoader(dev_dataset, batch_size=30, shuffle=True, collate_fn = MyCollate(pad_idx = train_dataset.pad_token_id, sep_idx = train_dataset.pad_token_id, max_width = 2, label_to_ind=label_to_ind))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c788cfca42684e7ba13c2e19fa0994f4",
            "df09b82361e745c2b3113e16f02fd122",
            "b6fe7d8b51f242db8fe82f1fefe551fb",
            "88b19677c26e462787c969160311ecd9",
            "791108d22ce14f4fb98bbc40152e2f65",
            "7118b27637d349349995877bd15390f7",
            "bffb7bb433b04a8e937ebd8d8a8b9849",
            "eeb293f53b8847f0b00a29dfd872a175",
            "33cd968357bd47e7b05ef69af167a166",
            "9da1ad4903f5441ca68642b3fd79894f",
            "d5ca51e9fb6442ed84acbfa3d2d8a159"
          ]
        },
        "id": "hiMHwONyljQi",
        "outputId": "e40ecc89-e051-46d6-9e1c-cb0a1163a853"
      },
      "outputs": [],
      "source": [
        "for batch_idx, x in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "      #print(x[\"sentence_mask\"], x[\"input_ids\"],x[\"attention_mask\"], x[\"label_ids\"])\n",
        "      print(x[\"sentence_mask\"].shape, x[\"input_ids\"].shape,x[\"attention_mask\"].shape, x[\"label_ids\"].shape, x[\"doc_name\"],x[\"span_indices\"].shape,x[\"segment_mask\"].shape)\n",
        "      #print(x['span_indices'])\n",
        "      print(x['label_ids'])\n",
        "      print(x[\"segment_mask\"])\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjoUAB9FD4Dj"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class SpanCRF(nn.Module):\n",
        "    def __init__(self,  label_to_ind, max_path):\n",
        "        super(SpanCRF, self).__init__()\n",
        "\n",
        "        self.tag_to_ix = label_to_ind\n",
        "        self.tagset_size = len(self.tag_to_ix)\n",
        "        self.max_path = max_path\n",
        "        \n",
        "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "\n",
        "        \n",
        "    def _forward_alg(self, logits, len_list, is_volatile=False):\n",
        "        \"\"\"\n",
        "        Computes the (batch_size,) denominator term (FloatTensor list) for the log-likelihood, which is the\n",
        "        sum of the likelihoods across all possible state sequences.\n",
        "        \n",
        "        Arguments:\n",
        "            logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n",
        "            lens: [batch_size] LongTensor\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, max_path, n_labels = logits.size()\n",
        "        \n",
        "        alpha = logits.data.new(batch_size, seq_len+1, self.tagset_size).fill_(-10000).to(device)\n",
        "        alpha[:, 0, self.tag_to_ix['START']] = 0\n",
        "        alpha = Variable(alpha, volatile=is_volatile)\n",
        "        \n",
        "        # Transpose batch size and time dimensions:\n",
        "        logits_t = logits.permute(1,0,2,3).to(device)\n",
        "        c_lens = len_list.clone()\n",
        "        \n",
        "        alpha_out_sum = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(0)).to(device)\n",
        "        mat = Variable(logits.data.new(batch_size,self.tagset_size,self.tagset_size).fill_(0)).to(device)\n",
        "        \n",
        "        for j, logit in enumerate(logits_t):\n",
        "            for i in range(0,max_path):\n",
        "                if i<=j:\n",
        "                    alpha_exp = alpha[:,j-i, :].clone().unsqueeze(1).expand(batch_size,self.tagset_size, self.tagset_size)\n",
        "                    logit_exp = logit[:, i].unsqueeze(-1).expand(batch_size, self.tagset_size, self.tagset_size).to(device)\n",
        "                    trans_exp = self.transitions.unsqueeze(0).expand_as(alpha_exp)\n",
        "                    mat = alpha_exp + logit_exp + trans_exp\n",
        "                    alpha_out_sum[:,i,:] =  self.log_sum_exp(mat , 2, keepdim=True).squeeze(2)\n",
        "\n",
        "            alpha_nxt = self.log_sum_exp(alpha_out_sum , dim=1, keepdim=True).squeeze(1)\n",
        "            \n",
        "            mask = Variable((c_lens > 0).float().unsqueeze(-1).expand(batch_size,self.tagset_size)).to(device)\n",
        "            alpha_nxt = mask * alpha_nxt + (1 - mask) *alpha[:, j, :].clone() \n",
        "            \n",
        "            c_lens = c_lens - 1      \n",
        "\n",
        "            alpha[:,j+1, :] = alpha_nxt\n",
        "\n",
        "        alpha[:,-1,:] = alpha[:,-1,:] + self.transitions[self.tag_to_ix['STOP']].unsqueeze(0).expand_as(alpha[:,-1,:])\n",
        "        norm = self.log_sum_exp(alpha[:,-1,:], 1).squeeze(-1)\n",
        "\n",
        "        return norm\n",
        "\n",
        "        \n",
        "    def viterbi_decode(self, logits, lens):\n",
        "        \"\"\"\n",
        "        Use viterbi algorithm to compute the most probable path of segments\n",
        "        \n",
        "        Arguments:\n",
        "            logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n",
        "            lens: [batch_size] LongTensor\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, max_path, n_labels = logits.size()\n",
        "        logits = logits.to(device)\n",
        "        # Transpose to batch size and time dimensions\n",
        "        logits_t = logits.permute(1,0,2,3)\n",
        "        \n",
        "        vit = Variable(logits.data.new(batch_size,seq_len+1, self.tagset_size).fill_(-10000),\n",
        "                                       volatile = not self.training).to(device)\n",
        "        \n",
        "        vit_tag_max = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(-10000),\n",
        "                                   volatile = not self.training).to(device)\n",
        "        \n",
        "        vit_tag_argmax = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(-100),\n",
        "                                   volatile = not self.training).to(device)\n",
        "        vit[:,0, self.tag_to_ix['START']] = 0\n",
        "        c_lens = Variable(lens.clone(), volatile= not self.training).to(device)\n",
        "        \n",
        "        pointers = Variable(logits.data.new(batch_size, seq_len, self.tagset_size, 2 ).fill_(-100))\n",
        "        for j, logit in enumerate(logits_t):\n",
        "            for i in range(0,max_path):\n",
        "                if i<=j:\n",
        "                    vit_exp = vit[:,j-i, :].clone().unsqueeze(1).expand(batch_size,self.tagset_size, self.tagset_size)\n",
        "                    trn_exp = self.transitions.unsqueeze(0).expand_as(vit_exp)\n",
        "                    vit_trn_sum = vit_exp + trn_exp\n",
        "                    vt_max, vt_argmax = vit_trn_sum.max(2)\n",
        "                    vit_nxt = vt_max + logit[:, i]\n",
        "                    vit_tag_max[:,i,:] = vit_nxt\n",
        "                    vit_tag_argmax[:,i,:] = vt_argmax\n",
        "           \n",
        "            seg_vt_max, seg_vt_argmax = vit_tag_max.max(1)\n",
        "            \n",
        "            mask = (c_lens > 0).float().unsqueeze(-1).expand_as(seg_vt_max)\n",
        "            vit[:, j+1, :] = mask*seg_vt_max + (1-mask)*vit[:, j, :].clone()\n",
        "            \n",
        "            mask = (c_lens == 1).float().unsqueeze(-1).expand_as(  vit[:, j+1, :])\n",
        "            vit[:, j+1, :] = vit[:, j+1, :] +  mask * self.transitions[ self.tag_to_ix['STOP'] ].unsqueeze(0).expand_as( vit[:, j+1, :] )\n",
        "            \n",
        "            idx_exp = seg_vt_argmax.unsqueeze(1)\n",
        "            pointers[:,j,:,0] =  torch.gather(vit_tag_argmax, 1,idx_exp ).squeeze(1)\n",
        "            pointers[:,j,:,1] = seg_vt_argmax \n",
        "            \n",
        "            c_lens = c_lens - 1  \n",
        "        \n",
        "        #Get the argmax from the last viterbi scores and follow the reverse pointers for the best path \n",
        "        end_max , end_max_idx = vit[:,-1,:].max(1)\n",
        "        end_max_idx = end_max_idx.data.cpu().numpy()\n",
        "        \n",
        "        pointers = pointers.data.long().cpu().numpy()\n",
        "        pointers_rev = np.flip(pointers,1)\n",
        "        paths = []\n",
        "        segments = []\n",
        "        \n",
        "        for b in range(batch_size):\n",
        "            #Different lengths each sentence, so get the starting index on the reverse list\n",
        "            start_index = seq_len-lens[b] \n",
        "            path = [end_max_idx[b]]\n",
        "            segment = [lens[b]]\n",
        "            \n",
        "            if (start_index >= seq_len -1):\n",
        "                paths.append(path)\n",
        "                continue\n",
        "            \n",
        "            max_tuple = pointers_rev[b,start_index,end_max_idx[b]]\n",
        "            start_index += 1\n",
        "            prev_tag = end_max_idx[b]\n",
        "            next_tag = max_tuple[0]\n",
        "            next_jump = max_tuple[1]\n",
        "            \n",
        "            for j, argmax in enumerate(pointers_rev[b,start_index:,:]):\n",
        "                #Append same tag as many times as indicated by the best segment length we stored\n",
        "                if next_jump > 0:\n",
        "                    next_jump -= 1\n",
        "                    path.insert(0, prev_tag)\n",
        "                    continue\n",
        "                #Switch to next tag when we hit zero\n",
        "                else:\n",
        "                    segment.insert(0, lens[b]- j-1)\n",
        "                    path.insert(0, next_tag)\n",
        "                \n",
        "                #Get the next tag, and the number of times we have to append the previous one\n",
        "                prev_tag = next_tag\n",
        "                max_tuple = argmax[next_tag]\n",
        "                next_tag = max_tuple[0]\n",
        "                next_jump = max_tuple[1]\n",
        "                \n",
        "            segments.append(segment)     \n",
        "            paths.append(path)\n",
        "            \n",
        "        return paths, segments\n",
        "        \n",
        "        \n",
        "    def _bilstm_score(self, logits, labels, seg_inds, lens):\n",
        "        \n",
        "        \"\"\"\n",
        "        Computes the (batch_size,) numerator (FloatTensor list) for the log-likelihood, which is the\n",
        "        \n",
        "        Arguments:\n",
        "            logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n",
        "            labels: [batch_size, seq_len] LongTensor\n",
        "            seg_inds: [batch_size, seq_len] LongTensor\n",
        "            lens: [batch_size] LongTensor\n",
        "        \"\"\"\n",
        "        lens = Variable( lens, volatile = not self.training)\n",
        "        \n",
        "        batch_size, max_len, _, _ = logits.size()\n",
        "        \n",
        "        # Transpose to batch size and time dimensions\n",
        "        labels = labels.transpose(1,0)\n",
        "        \n",
        "        seg_inds = seg_inds.transpose(1,0).data.cpu().numpy()\n",
        "        labels_exp = labels.unsqueeze(-1)\n",
        "\n",
        "        #Construct the mask the will sellect the corrects segments from all possible segments for each timstep\n",
        "        mask_seg = np.zeros(( batch_size, max_len, self.max_path))\n",
        "        \n",
        "        mask_step =  np.zeros(( batch_size), dtype=np.int32)\n",
        "        counter = np.zeros((batch_size), dtype=np.int32)\n",
        "        \n",
        "        #For each timstep accross all sentences\n",
        "        for i in range(0,max_len):\n",
        "            #0 or 1 depending if we are on the end of a segment\n",
        "            mask_step =  seg_inds[:, i] \n",
        "            mask_seg[np.arange(batch_size), i, counter] = mask_step \n",
        "            counter = counter + 1\n",
        "            counter = (1- mask_step)*counter*(counter < self.max_path)\n",
        "           \n",
        "        mask_seg = torch.from_numpy(mask_seg).float()\n",
        "        if next(self.parameters()).is_cuda == True:\n",
        "            mask_seg = mask_seg.cuda()\n",
        "            \n",
        "        mask_seg = mask_seg.unsqueeze(-1).expand_as(logits)\n",
        "        mask_seg = Variable(mask_seg,  volatile = not self.training).to(device)\n",
        "        \n",
        "        logit_mask = logits*mask_seg\n",
        "        sum_cols = torch.sum(logit_mask, dim=2).squeeze(2)\n",
        "        \n",
        "        all_scores = torch.gather(sum_cols, 2, labels_exp).squeeze(-1)\n",
        "        \n",
        "        mask_time = self.sequence_mask(lens).float()\n",
        "        all_scores = all_scores*mask_time\n",
        "        \n",
        "        sum_seg_scores = torch.sum(all_scores, dim=1).squeeze(-1)\n",
        "\n",
        "        return  sum_seg_scores\n",
        "        \n",
        "    def score(self, logits, y, seg_inds, lens):\n",
        "        logits = logits.to(device)\n",
        "        bilstm_score = self._bilstm_score(logits, y, seg_inds, lens)\n",
        "        transition_score = self.transition_score(y, lens, seg_inds )\n",
        "        \n",
        "        score = transition_score + bilstm_score\n",
        "\n",
        "        return score\n",
        "    \n",
        "    def transition_score(self, labels, lens, mask_seg_idx):\n",
        "        \"\"\"\n",
        "        Computes the (batch_size,) scores (FloatTensor list) that will be added to the emission scores\n",
        "        \n",
        "        Arguments:\n",
        "            logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n",
        "            labels: [batch_size, seq_len] LongTensor\n",
        "            seg_inds: [batch_size, seq_len] LongTensor\n",
        "            lens: [batch_size] LongTensor\n",
        "        \"\"\"\n",
        "        lens = Variable( lens, volatile = not self.training)\n",
        "        labels = labels.transpose(1,0)\n",
        "        mask_seg_idx = mask_seg_idx.transpose(1,0)\n",
        "        batch_size, seq_len = labels.size()\n",
        "        # pad labels with <start> and <stop> indices\n",
        "        labels_ext = Variable(labels.data.new(batch_size, seq_len + 2))\n",
        "        labels_ext[:, 0] = self.tag_to_ix['START']\n",
        "        labels_ext[:, 1:-1] = labels\n",
        "        mask = self.sequence_mask(lens + 1, max_len=seq_len + 2).long()\n",
        "        pad_stop = Variable(labels.data.new(1).fill_(self.tag_to_ix['STOP']))\n",
        "        \n",
        "        pad_stop = pad_stop.unsqueeze(-1).expand(batch_size, seq_len + 2)\n",
        "        labels_ext = (1 + (-1)*mask) * pad_stop + mask * labels_ext\n",
        "        trn = self.transitions\n",
        "        \n",
        "        trn_exp = trn.unsqueeze(0).expand(batch_size, *trn.size())\n",
        "        lbl_r = labels_ext[:, 1:]\n",
        "        lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), trn.size(0))\n",
        "        trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n",
        "        \n",
        "        lbl_lexp = labels_ext[:, :-1].unsqueeze(-1)\n",
        "        trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n",
        "        trn_scr = trn_scr.squeeze(-1)\n",
        "        \n",
        "        # Mask sentences in time dim\n",
        "        mask = self.sequence_mask(lens + 1).float()\n",
        "        trn_scr = trn_scr * mask\n",
        "        \n",
        "        trn_scr[:, 1:] = trn_scr[:, 1:].clone()*mask_seg_idx.float() \n",
        "        \n",
        "        score = trn_scr.sum(1).squeeze(-1)\n",
        "        \n",
        "        return score\n",
        "\n",
        "    def loglik(self, logits, y, lens):\n",
        "        norm_score = self._forward_alg(logits, lens)\n",
        "        sequence_score = self.score(logits, y, lens, logits=logits)\n",
        "        loglik = sequence_score - norm_score\n",
        "\n",
        "        return loglik   \n",
        "\n",
        "\n",
        "    def log_sum_exp(self,vec, dim=0, keepdim=True):\n",
        "        max_val, idx = torch.max(vec, dim, keepdim=True)\n",
        "        max_exp = max_val.expand_as(vec)\n",
        "    \n",
        "        return max_val + torch.log(torch.sum(torch.exp(vec - max_exp), dim, keepdim=keepdim))\n",
        "\n",
        "    \n",
        "    def sequence_mask(self,lens, max_len=None):\n",
        "        batch_size = lens.size(0)\n",
        "        if max_len is None:\n",
        "        \n",
        "            max_len = lens.max().data\n",
        "            \n",
        "        ranges = torch.arange(0, max_len).long()\n",
        "        ranges = ranges.unsqueeze(0).expand(batch_size, max_len)\n",
        "        ranges = Variable(ranges)\n",
        "        if lens.data.is_cuda:\n",
        "            ranges = ranges.cuda()\n",
        "\n",
        "        lens_exp = lens.unsqueeze(1).expand_as(ranges)\n",
        "        mask = ranges < lens_exp\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCrxoxXjHJbx"
      },
      "outputs": [],
      "source": [
        "from allennlp.common.util import pad_sequence_to_length\n",
        "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import masked_mean, masked_softmax\n",
        "from allennlp.modules.span_extractors import EndpointSpanExtractor,SelfAttentiveSpanExtractor\n",
        "import copy\n",
        "\n",
        "from transformers import BertModel\n",
        "\n",
        "from allennlp.modules import ConditionalRandomField\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CRFOutputLayer(torch.nn.Module):\n",
        "    ''' CRF output layer consisting of a linear layer and a CRF. '''\n",
        "    def __init__(self, in_dim, num_labels):\n",
        "        super(CRFOutputLayer, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.classifier = torch.nn.Linear(in_dim, self.num_labels)\n",
        "        self.crf = ConditionalRandomField(self.num_labels)\n",
        "\n",
        "    def forward(self, x, mask, labels=None):\n",
        "        ''' x: shape: batch, max_sequence, in_dim\n",
        "            mask: shape: batch, max_sequence\n",
        "            labels: shape: batch, max_sequence\n",
        "        '''\n",
        "\n",
        "        batch_size, max_sequence, in_dim = x.shape\n",
        "\n",
        "        logits = self.classifier(x)\n",
        "        outputs = {}\n",
        "        if labels is not None:\n",
        "            log_likelihood = self.crf(logits, labels, mask)\n",
        "            loss = -log_likelihood\n",
        "            outputs[\"loss\"] = loss\n",
        "        else:\n",
        "            best_paths = self.crf.viterbi_tags(logits, mask)\n",
        "            predicted_label = [x for x, y in best_paths]\n",
        "            predicted_label = [pad_sequence_to_length(x, desired_length=max_sequence) for x in predicted_label]\n",
        "            predicted_label = torch.tensor(predicted_label)\n",
        "            outputs[\"predicted_label\"] = predicted_label\n",
        "\n",
        "            #log_denominator = self.crf._input_likelihood(logits, mask)\n",
        "            #log_numerator = self.crf._joint_likelihood(logits, predicted_label, mask)\n",
        "            #log_likelihood = log_numerator - log_denominator\n",
        "            #outputs[\"log_likelihood\"] = log_likelihood\n",
        "\n",
        "        return outputs\n",
        "        \n",
        "\n",
        "\n",
        "class AttentionPooling(torch.nn.Module):\n",
        "    def __init__(self, in_features, dimension_context_vector_u=200, number_context_vectors=5):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.dimension_context_vector_u = dimension_context_vector_u\n",
        "        self.number_context_vectors = number_context_vectors\n",
        "        self.linear1 = torch.nn.Linear(in_features=in_features, out_features=self.dimension_context_vector_u, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(in_features=self.dimension_context_vector_u,\n",
        "                                       out_features=self.number_context_vectors, bias=False)\n",
        "\n",
        "        self.output_dim = self.number_context_vectors * in_features\n",
        "\n",
        "    def forward(self, tokens, mask):\n",
        "        #shape tokens: (batch_size, tokens, in_features)\n",
        "\n",
        "        # compute the weights\n",
        "        # shape tokens: (batch_size, tokens, dimension_context_vector_u)\n",
        "        a = self.linear1(tokens)\n",
        "        a = torch.tanh(a)\n",
        "        # shape (batch_size, tokens, number_context_vectors)\n",
        "        a = self.linear2(a)\n",
        "        # shape (batch_size, number_context_vectors, tokens)\n",
        "        a = a.transpose(1, 2)\n",
        "        a = masked_softmax(a, mask)\n",
        "\n",
        "        # calculate weighted sum\n",
        "        s = torch.bmm(a, tokens)\n",
        "        s = s.view(tokens.shape[0], -1)\n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "class BertTokenEmbedder(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertTokenEmbedder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(config[\"bert_model\"])\n",
        "        self.bert_trainable = config[\"bert_trainable\"]\n",
        "        self.bert_hidden_size = self.bert.config.hidden_size\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = self.bert_trainable\n",
        "\n",
        "    def forward(self, batch):\n",
        "        if \"bert_embeddings\" in batch:\n",
        "            return batch[\"bert_embeddings\"]\n",
        "\n",
        "        documents, sentences, tokens = batch[\"input_ids\"].shape\n",
        "        attention_mask = batch[\"attention_mask\"].view(-1, tokens)\n",
        "        input_ids = batch[\"input_ids\"].view(-1, tokens)\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # shape (documents*sentences, tokens, 768)\n",
        "        bert_embeddings = outputs[0]\n",
        "\n",
        "        if not self.bert_trainable:\n",
        "            batch[\"bert_embeddings\"] = bert_embeddings.to(\"cpu\")\n",
        "        return bert_embeddings\n",
        "\n",
        "class BertHSLN(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertHSLN, self).__init__()\n",
        "\n",
        "        self.bert = BertTokenEmbedder(config)\n",
        "        self.dropout = torch.nn.Dropout(config[\"dropout\"])\n",
        "        self.word_lstm_hidden_size = config[\"word_lstm_hs\"]\n",
        "        self.word_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(input_size=self.bert.bert_hidden_size,\n",
        "                                  hidden_size=self.word_lstm_hidden_size,\n",
        "                                  num_layers=1, batch_first=True, bidirectional=True))\n",
        "\n",
        "        self.attention_pooling = AttentionPooling(2 * self.word_lstm_hidden_size,\n",
        "                                                  dimension_context_vector_u=config[\"att_pooling_dim_ctx\"],\n",
        "                                                  number_context_vectors=config[\"att_pooling_num_ctx\"])\n",
        "        \n",
        "        input_dim = self.attention_pooling.output_dim\n",
        "        self.sentence_lstm_hidden_size = config[\"sentence_lstm_hs\"]\n",
        "        self.sentence_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(input_size=input_dim,\n",
        "                                  hidden_size=self.sentence_lstm_hidden_size,\n",
        "                                  num_layers=1, batch_first=True, bidirectional=True))\n",
        "        \n",
        "\n",
        "\n",
        "        self.input_dim = self.sentence_lstm_hidden_size * 2\n",
        "        self.max_path = config[\"max_path\"]\n",
        "        self.num_labels = len(config['label_to_ind'])\n",
        "        \n",
        "        self.span_crf = config[\"span_crf\"]\n",
        "        self.crf = config[\"crf\"]\n",
        "\n",
        "        if self.crf:\n",
        "          self.crf_fc = nn.Linear(self.input_dim, self.num_labels)\n",
        "          self.crf = SpanCRF(config[\"label_to_ind\"],1)\n",
        "          #self.crf = CRFOutputLayer(in_dim=self.input_dim, num_labels=self.num_labels)\n",
        "\n",
        "        if self.span_crf:\n",
        "          #self.endpoint_span_extractor = EndpointSpanExtractor(self.sentence_lstm_hidden_size * 2,\n",
        "          #                                                    combination=\"x,y,x*y,x-y\",\n",
        "          #                                                    num_width_embeddings=config[\"max_path\"],\n",
        "          #                                                    span_width_embedding_dim=config[\"span_width_embedding_dim\"],\n",
        "          #                                                    bucket_widths=True)\n",
        "          self.endpoint_span_extractor = SelfAttentiveSpanExtractor(self.sentence_lstm_hidden_size * 2,\n",
        "                                                              num_width_embeddings=config[\"max_path\"],\n",
        "                                                              span_width_embedding_dim=config[\"span_width_embedding_dim\"],\n",
        "                                                              bucket_widths=True)\n",
        "          #self.span_input_dim = self.sentence_lstm_hidden_size * 2 * 4 + config[\"span_width_embedding_dim\"]\n",
        "          self.span_input_dim = self.sentence_lstm_hidden_size * 2  + config[\"span_width_embedding_dim\"]\n",
        "          \n",
        "          self.crf_spanfc = nn.Linear(self.span_input_dim, self.num_labels)\n",
        "          self.spancrf = SpanCRF(config[\"label_to_ind\"],self.max_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, batch, labels=None, eval=False):\n",
        "\n",
        "        documents, sentences, tokens = batch[\"input_ids\"].shape\n",
        "  \n",
        "        # shape (documents*sentences, tokens, 768)\n",
        "        bert_embeddings = self.bert(batch)\n",
        "        bert_embeddings = self.dropout(bert_embeddings)\n",
        "\n",
        "        tokens_mask = batch[\"attention_mask\"].view(-1, tokens)\n",
        "\n",
        "        # shape (documents*sentences, tokens, 2*lstm_hidden_size)\n",
        "        bert_embeddings_encoded = self.word_lstm(bert_embeddings, tokens_mask)\n",
        "\n",
        "        #shape (documents*sentences, pooling_out)\n",
        "        sentence_embeddings = self.attention_pooling(bert_embeddings_encoded, tokens_mask)\n",
        "\n",
        "        # shape: (documents, sentences, pooling_out)\n",
        "        sentence_embeddings = sentence_embeddings.view(documents, sentences, -1)\n",
        "        sentence_embeddings = self.dropout(sentence_embeddings)\n",
        "\n",
        "\n",
        "        sentence_mask = batch[\"sentence_mask\"]\n",
        "\n",
        "        # shape: (documents, sentence, 2*lstm_hidden_size)\n",
        "        sentence_embeddings_encoded = self.sentence_lstm(sentence_embeddings, sentence_mask)\n",
        "        sentence_embeddings_encoded = self.dropout(sentence_embeddings_encoded)\n",
        "\n",
        "        sentence_len = torch.sum(sentence_mask,dim=-1)\n",
        "        output = {}\n",
        "\n",
        "        if self.span_crf:\n",
        "            span_embeddings = self.endpoint_span_extractor(sentence_embeddings_encoded,batch[\"span_indices\"], sentence_mask)\n",
        "            segment_rep = self.crf_spanfc(span_embeddings)\n",
        "            _,max_span_len,_ = segment_rep.shape\n",
        "        \n",
        "            segment_span_feat = torch.zeros(documents, sentences, self.max_path, self.num_labels)\n",
        "\n",
        "        \n",
        "            batch_size, max_span_len,_ = batch[\"span_indices\"].shape\n",
        "            _, max_seq_len, max_path_len, _ = segment_span_feat.shape\n",
        "\n",
        "            for i in range(batch_size):\n",
        "              for j in range(max_span_len):\n",
        "                start_idx = batch[\"span_indices\"][i][j][0]\n",
        "                len_idx = batch[\"span_indices\"][i][j][1] - batch[\"span_indices\"][i][j][0]\n",
        "                segment_span_feat[i,start_idx,len_idx,:] = segment_rep[i][j]\n",
        "            \n",
        "            segment_mask = batch[\"segment_mask\"]\n",
        "            \n",
        "            if not eval:\n",
        "                span_forward_var_batch = self.spancrf._forward_alg(segment_span_feat,sentence_len )\n",
        "                span_gold_score_batch = self.spancrf.score(segment_span_feat, labels.transpose(0,1) , segment_mask.transpose(0,1),sentence_len)\n",
        "                loss = (span_forward_var_batch-span_gold_score_batch).mean()\n",
        "                #output['span_crf'] = {\"forward_var_batch\":span_forward_var_batch , \"gold_score_batch\" : span_gold_score_batch}\n",
        "                output['loss2'] = loss\n",
        "\n",
        "        if self.crf:\n",
        "            #output = self.crf(sentence_embeddings_encoded, sentence_mask, labels)\n",
        "            #return output \n",
        "\n",
        "            segment_feat = sentence_embeddings_encoded.unsqueeze(2)\n",
        "            segment_feat = self.crf_fc(segment_feat)\n",
        "            segment_feat = segment_feat.view(documents, sentences, 1, self.num_labels)\n",
        "            \n",
        "            if not eval:\n",
        "                forward_var_batch = self.crf._forward_alg(segment_feat,sentence_len )\n",
        "                gold_score_batch = self.crf.score(segment_feat, labels.transpose(0,1) , sentence_mask.transpose(0,1),sentence_len)\n",
        "                loss = (forward_var_batch-gold_score_batch  ).mean()\n",
        "                output['loss1'] = loss\n",
        "            \n",
        "\n",
        "        if eval:\n",
        "            if self.crf:\n",
        "              crf_tag_seqs, crf_segments = self.crf.viterbi_decode(segment_feat,sentence_len )\n",
        "              #output['crf'] = {\"tag_seqs\":crf_tag_seqs, \"segments\":  crf_segments }\n",
        "              output[\"predicted_label1\"] = crf_tag_seqs\n",
        "            if self.span_crf:\n",
        "              span_crf_tag_seqs, span_crf_segments = self.spancrf.viterbi_decode(segment_span_feat,sentence_len )\n",
        "              #output['span_crf'] = {\"tag_seqs\":span_crf_tag_seqs, \"segments\":  span_crf_segments }\n",
        "              output[\"predicted_label2\"] = span_crf_tag_seqs\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_kWu8asfteX"
      },
      "outputs": [],
      "source": [
        "#MAX PATH CHANGED TO MAX WIDTH \n",
        "\n",
        "config = {\n",
        "    \"dropout\":0.5,\n",
        "    \"word_lstm_hs\":758,\n",
        "    \"att_pooling_dim_ctx\":200,\n",
        "    \"att_pooling_num_ctx\": 15,\n",
        "    \"sentence_lstm_hs\":758,\n",
        "    \"bert_model\": bert_model,\n",
        "    \"bert_trainable\": False,\n",
        "    \"label_to_ind\" : label_to_ind,\n",
        "    \"max_path\": 2,\n",
        "    \"span_width_embedding_dim\" :100,\n",
        "    \"lr_epoch_decay\":0.9,\n",
        "    \"crf\": False,\n",
        "    \"span_crf\" : True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH6iJd1GJocl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def eval_model(model, eval_batches, device,label_to_ind, id):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    labels_dict={}\n",
        "    predicted_labels = []\n",
        "    docwise_predicted_labels=[]\n",
        "    docwise_true_labels = []\n",
        "    doc_name_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_batches:\n",
        "            # move tensor to gpu\n",
        "            shift_to_device(batch,device)\n",
        "\n",
        "            output = model(batch=batch, eval=True)\n",
        "            \n",
        "            for each in range(len(batch['doc_name'])):\n",
        "                true_labels_batch, predicted_labels_batch = \\\n",
        "                    clear_and_map_padded_values(batch[\"label_ids\"][each], output[id][each],label_to_ind)\n",
        "                assert len(batch['sentence_mask'][each].nonzero()) == len(predicted_labels_batch)\n",
        "\n",
        "                #print(batch[\"label_ids\"][each].shape, output['predicted_label'][each].shape)\n",
        "                #print(len(true_labels_batch), len(predicted_labels_batch))\n",
        "                #print(batch['doc_name'][each])\n",
        "                docwise_true_labels.append(true_labels_batch)\n",
        "                docwise_predicted_labels.append(predicted_labels_batch)\n",
        "                doc_name_list.append(batch['doc_name'][each])\n",
        "                true_labels.extend(true_labels_batch)\n",
        "                predicted_labels.extend(predicted_labels_batch)\n",
        "            \n",
        "            shift_to_device(batch,torch.device(\"cpu\"))\n",
        "    \n",
        "    labels_dict['y_true']=true_labels\n",
        "    labels_dict['y_predicted'] = predicted_labels\n",
        "    labels_dict['docwise_y_true'] = docwise_true_labels\n",
        "    labels_dict['docwise_y_predicted'] = docwise_predicted_labels\n",
        "    labels_dict['doc_names'] = doc_name_list\n",
        "    metrics, confusion, class_report = \\\n",
        "        calc_classification_metrics(y_true=true_labels, y_predicted=predicted_labels,labels = list(label_to_ind.keys()))\n",
        "    return metrics, confusion,labels_dict, class_report\n",
        "    \n",
        "    \n",
        "def clear_and_map_padded_values(true_labels, predicted_labels,label_to_ind):\n",
        "    cleared_predicted = []\n",
        "    cleared_true = []\n",
        "    ind_to_label = {v: k for k, v in label_to_ind.items()}\n",
        "    for true_label, predicted_label in zip(true_labels, predicted_labels):\n",
        "        if true_label.item() != label_to_ind['MASK']:\n",
        "            cleared_true.append(ind_to_label[true_label.item()])\n",
        "            cleared_predicted.append(ind_to_label[predicted_label.item()])\n",
        "    return cleared_true, cleared_predicted\n",
        "\n",
        "def calc_span_idx(labels):\n",
        "    span_idx = []\n",
        "    span_labels = []\n",
        "    i = 0\n",
        "    while(i < len(labels)):\n",
        "      start = 0\n",
        "      end = 0\n",
        "      if(i == len(labels) - 1):\n",
        "        if(labels[i] != labels[i-1]):\n",
        "          start = i\n",
        "          end = i\n",
        "          i = i + 1\n",
        "      else:\n",
        "        if(labels[i] != labels[i+1]):\n",
        "          start = i\n",
        "          end = i\n",
        "          i = i + 1\n",
        "        else:\n",
        "          start = i\n",
        "          while(labels[i] == labels[i+1]):\n",
        "            i = i + 1\n",
        "            if(i == len(labels) - 1):\n",
        "              end = i\n",
        "              break\n",
        "          end = i\n",
        "          i = i + 1\n",
        "\n",
        "      span_idx.append((start, end))\n",
        "      span_labels.append(labels[start])\n",
        "\n",
        "    return span_idx, span_labels\n",
        "\n",
        "\n",
        "def calc_classification_metrics(y_true, y_predicted, labels):\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_predicted, average='macro')\n",
        "    micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(y_true, y_predicted, average='micro')\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_true, y_predicted, average='weighted')\n",
        "    per_label_precision, per_label_recall, per_label_f1, _ = precision_recall_fscore_support(y_true, y_predicted, average=None, labels=labels)\n",
        "\n",
        "    true_spans, t_labels = calc_span_idx(y_true)\n",
        "    pred_spans, p_labels = calc_span_idx(y_predicted)\n",
        "    \n",
        "    span_result = [1 for i, j in zip(true_spans, pred_spans) if i == j]\n",
        "\n",
        "    span_F1 = len(span_result) / len(true_spans)\n",
        "    \n",
        "\n",
        "    acc = accuracy_score(y_true, y_predicted)\n",
        "\n",
        "    class_report = classification_report(y_true, y_predicted, digits=4)\n",
        "    confusion_abs = confusion_matrix(y_true, y_predicted, labels=labels)\n",
        "    # normalize confusion matrix\n",
        "    confusion = np.around(confusion_abs.astype('float') / confusion_abs.sum(axis=1)[:, np.newaxis] * 100, 2)\n",
        "    return {\"acc\": acc,\n",
        "            \"macro-f1\": macro_f1,\n",
        "            \"Span-F1\": span_F1,\n",
        "            \"macro-precision\": macro_precision,\n",
        "            \"macro-recall\": macro_recall,\n",
        "            \"micro-f1\": micro_f1,\n",
        "            \"micro-precision\": micro_precision,\n",
        "            \"micro-recall\": micro_recall,\n",
        "            \"weighted-f1\": weighted_f1,\n",
        "            \"weighted-precision\": weighted_precision,\n",
        "            \"weighted-recall\": weighted_recall,\n",
        "            \"labels\": labels,\n",
        "            \"per-label-f1\": per_label_f1.tolist(),\n",
        "            \"per-label-precision\": per_label_precision.tolist(),\n",
        "            \"per-label-recall\": per_label_recall.tolist(),\n",
        "            \"confusion_abs\": confusion_abs.tolist()\n",
        "            }, \\\n",
        "           confusion.tolist(), \\\n",
        "           class_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TlmMAabCjMP"
      },
      "outputs": [],
      "source": [
        "def shift_to_device(batch,device):\n",
        "  for key in batch.keys():\n",
        "    if(torch.is_tensor(batch[key])):\n",
        "      batch[key] = batch[key].to(device)\n",
        "    else:\n",
        "      batch[key] = batch[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o36lYyNQDZ4i",
        "outputId": "200480aa-d572-4738-fc7f-eb4763ae57d0"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import itertools\n",
        "import gc\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_epochs = 1\n",
        "learning_rate = 3e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BertHSLN(config).to(device)\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if(\"bert\" in name):\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, model.parameters())), lr=learning_rate)\n",
        "max_grad_norm = 1.0\n",
        "epoch_scheduler = StepLR(optimizer, step_size=1, gamma=config[\"lr_epoch_decay\"])\n",
        "\n",
        "accs = []\n",
        "epochs = []\n",
        "train_losses = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "    model.train()\n",
        "    for batch_idx, batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "        shift_to_device(batch,device)\n",
        "        output = model(batch, batch[\"label_ids\"])\n",
        "        loss = output[\"loss2\"]\n",
        "        #loss = output[\"loss1\"] + output[\"loss2\"]\n",
        "        loss = loss.sum()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        shift_to_device(batch,torch.device(\"cpu\"))\n",
        "    \n",
        "    epochs.append(epoch)\n",
        "    train_losses.append(loss.item())\n",
        "    print(f\"Loss in epoch {epoch} : {loss.item()}\")\n",
        "    epoch_scheduler.step()\n",
        "\n",
        "    #test_metrics, test_confusion,labels_dict,_ = eval_model(model, dev_dataloader , device,label_to_ind,\"predicted_label1\")\n",
        "    #print(test_metrics)\n",
        "    test_metrics, test_confusion,labels_dict,_ = eval_model(model, dev_dataloader , device,label_to_ind,\"predicted_label2\")\n",
        "    accs.append(test_metrics['acc'])\n",
        "    print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "CUS-DMHZFMxK",
        "outputId": "b2cb25b5-6488-4abd-ca39-1cfd74244137"
      },
      "outputs": [],
      "source": [
        "#PahPre macroF1 0.32 acc: 0.58 Epoch: 25 span len: 2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs, train_losses)\n",
        "plt.ylabel('Train loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "lrEuX_TrGE_Q",
        "outputId": "3323bcc8-b9c3-41de-8772-e2b54dcff485"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, accs)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN1ZB0cH5rXS"
      },
      "outputs": [],
      "source": [
        "#Paheli macroF1 0.19777 acc: 0.42 Epoch: 31 span len: 10\n",
        "#PahPre macroF1 0.19371 acc: 0.42 Epoch: 50 span len: 10\n",
        "\n",
        "#PahPre macroF1 0.32 acc: 0.58 Epoch: 25 span len: 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFf2Y3pshr-V"
      },
      "outputs": [],
      "source": [
        "#Ekstep Span 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "voGLwBYjhsA8",
        "outputId": "ee67346d-8bdd-4169-85c4-658e07ac37f7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epochs, train_losses)\n",
        "plt.ylabel('Train loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "zt3AR9UbhsDs",
        "outputId": "afac577e-d52e-4e13-8afe-5a66fcd4f2b6"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, accs)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbckx4Xq5BGe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RRL_spanF1_new.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33cd968357bd47e7b05ef69af167a166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7118b27637d349349995877bd15390f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "791108d22ce14f4fb98bbc40152e2f65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b19677c26e462787c969160311ecd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da1ad4903f5441ca68642b3fd79894f",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ca51e9fb6442ed84acbfa3d2d8a159",
            "value": " 0/500 [00:00&lt;?, ?it/s]"
          }
        },
        "9da1ad4903f5441ca68642b3fd79894f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6fe7d8b51f242db8fe82f1fefe551fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeb293f53b8847f0b00a29dfd872a175",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33cd968357bd47e7b05ef69af167a166",
            "value": 0
          }
        },
        "bffb7bb433b04a8e937ebd8d8a8b9849": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c788cfca42684e7ba13c2e19fa0994f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df09b82361e745c2b3113e16f02fd122",
              "IPY_MODEL_b6fe7d8b51f242db8fe82f1fefe551fb",
              "IPY_MODEL_88b19677c26e462787c969160311ecd9"
            ],
            "layout": "IPY_MODEL_791108d22ce14f4fb98bbc40152e2f65"
          }
        },
        "d5ca51e9fb6442ed84acbfa3d2d8a159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df09b82361e745c2b3113e16f02fd122": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7118b27637d349349995877bd15390f7",
            "placeholder": "​",
            "style": "IPY_MODEL_bffb7bb433b04a8e937ebd8d8a8b9849",
            "value": "  0%"
          }
        },
        "eeb293f53b8847f0b00a29dfd872a175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
