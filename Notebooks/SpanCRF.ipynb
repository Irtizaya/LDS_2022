{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WZCf-yS3S6Jw"},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import numpy as np\n","from torch.autograd import Variable\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDETqFkBRw7o"},"outputs":[],"source":["class SCRF():\n","  def __init__(self, label_to_ind, max_path):\n","        super(SCRF, self).__init__()\n","            \n","        self.tag_to_ix = label_to_ind\n","        self.tagset_size = len(self.tag_to_ix)\n","        self.max_path = max_path\n","        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n","        \n","  def _forward_alg(self, logits, len_list, is_volatile=False):\n","        \"\"\"\n","        Computes the (batch_size,) denominator term (FloatTensor list) for the log-likelihood, which is the\n","        sum of the likelihoods across all possible state sequences.\n","        \n","        Arguments:\n","            logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n","            lens: [batch_size] LongTensor\n","        \"\"\"\n","        batch_size, seq_len, max_path, n_labels = logits.size()\n","        \n","        alpha = logits.data.new(batch_size, seq_len+1, self.tagset_size).fill_(-10000)\n","        alpha[:, 0, self.tag_to_ix['START']] = 0\n","        alpha = Variable(alpha, volatile=is_volatile)\n","        \n","        # Transpose batch size and time dimensions:\n","        logits_t = logits.permute(1,0,2,3)\n","        c_lens = len_list.clone()\n","        \n","        alpha_out_sum = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(0))\n","        mat = Variable(logits.data.new(batch_size,self.tagset_size,self.tagset_size).fill_(0))\n","        \n","        for j, logit in enumerate(logits_t):\n","            for i in range(0,max_path):\n","                if i<=j:\n","                    alpha_exp = alpha[:,j-i, :].clone().unsqueeze(1).expand(batch_size,self.tagset_size, self.tagset_size)\n","                    logit_exp = logit[:, i].unsqueeze(-1).expand(batch_size, self.tagset_size, self.tagset_size)\n","                    trans_exp = self.transitions.unsqueeze(0).expand_as(alpha_exp)\n","                    mat = alpha_exp + logit_exp + trans_exp\n","                    alpha_out_sum[:,i,:] =  self.log_sum_exp(mat , 2, keepdim=True)\n","                    \n","            alpha_nxt = self.log_sum_exp(alpha_out_sum , dim=1, keepdim=True).squeeze(1)\n","            \n","            mask = Variable((c_lens > 0).float().unsqueeze(-1).expand(batch_size,self.tagset_size))\n","            alpha_nxt = mask * alpha_nxt + (1 - mask) *alpha[:, j, :].clone() \n","            \n","            c_lens = c_lens - 1      \n","\n","            alpha[:,j+1, :] = alpha_nxt\n","\n","        alpha[:,-1,:] = alpha[:,-1,:] + self.transitions[self.tag_to_ix['STOP']].unsqueeze(0).expand_as(alpha[:,-1,:])\n","        norm = self.log_sum_exp(alpha[:,-1,:], 1).squeeze(-1)\n","\n","        return norm\n","  \n","\n","  def viterbi_decode(self, logits, lens):\n","      \"\"\"\n","      Use viterbi algorithm to compute the most probable path of segments\n","      \n","      Arguments:\n","          logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n","          lens: [batch_size] LongTensor\n","      \"\"\"\n","      batch_size, seq_len, max_path, n_labels = logits.size()\n","      \n","      # Transpose to batch size and time dimensions\n","      logits_t = logits.permute(1,0,2,3)\n","      \n","      vit = Variable(logits.data.new(batch_size,seq_len+1, self.tagset_size).fill_(-10000),\n","                                      volatile = not self.training)\n","      \n","      vit_tag_max = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(-10000),\n","                                  volatile = not self.training) \n","      \n","      vit_tag_argmax = Variable(logits.data.new(batch_size,max_path, self.tagset_size).fill_(-100),\n","                                  volatile = not self.training) \n","      vit[:,0, self.tag_to_ix['START']] = 0\n","      c_lens = Variable(lens.clone(), volatile= not self.training)\n","      \n","      pointers = Variable(logits.data.new(batch_size, seq_len, self.tagset_size, 2 ).fill_(-100))\n","      for j, logit in enumerate(logits_t):\n","          for i in range(0,max_path):\n","              if i<=j:\n","                  vit_exp = vit[:,j-i, :].clone().unsqueeze(1).expand(batch_size,self.tagset_size, self.tagset_size)\n","                  trn_exp = self.transitions.unsqueeze(0).expand_as(vit_exp)\n","                  vit_trn_sum = vit_exp + trn_exp\n","                  vt_max, vt_argmax = vit_trn_sum.max(2)\n","                  vit_nxt = vt_max + logit[:, i]\n","                  vit_tag_max[:,i,:] = vit_nxt\n","                  vit_tag_argmax[:,i,:] = vt_argmax\n","          \n","          seg_vt_max, seg_vt_argmax = vit_tag_max.max(1)\n","          \n","          mask = (c_lens > 0).float().unsqueeze(-1).expand_as(seg_vt_max)\n","          vit[:, j+1, :] = mask*seg_vt_max + (1-mask)*vit[:, j, :].clone()\n","          \n","          mask = (c_lens == 1).float().unsqueeze(-1).expand_as(  vit[:, j+1, :])\n","          vit[:, j+1, :] = vit[:, j+1, :] +  mask * self.transitions[ self.tag_to_ix['STOP'] ].unsqueeze(0).expand_as( vit[:, j+1, :] )\n","          \n","          idx_exp = seg_vt_argmax.unsqueeze(1)\n","          pointers[:,j,:,0] =  torch.gather(vit_tag_argmax, 1,idx_exp ).squeeze(1)\n","          pointers[:,j,:,1] = seg_vt_argmax \n","          \n","          c_lens = c_lens - 1  \n","      \n","      #Get the argmax from the last viterbi scores and follow the reverse pointers for the best path \n","      end_max , end_max_idx = vit[:,-1,:].max(1)\n","      end_max_idx = end_max_idx.data.cpu().numpy()\n","      \n","      pointers = pointers.data.long().cpu().numpy()\n","      pointers_rev = np.flip(pointers,1)\n","      paths = []\n","      segments = []\n","      \n","      for b in range(batch_size):\n","          #Different lengths each sentence, so get the starting index on the reverse list\n","          start_index = seq_len-lens[b] \n","          path = [end_max_idx[b]]\n","          segment = [lens[b]]\n","          \n","          if (start_index >= seq_len -1):\n","              paths.append(path)\n","              continue\n","          \n","          max_tuple = pointers_rev[b,start_index,end_max_idx[b]]\n","          start_index += 1\n","          prev_tag = end_max_idx[b]\n","          next_tag = max_tuple[0]\n","          next_jump = max_tuple[1]\n","          \n","          for j, argmax in enumerate(pointers_rev[b,start_index:,:]):\n","              #Append same tag as many times as indicated by the best segment length we stored\n","              if next_jump > 0:\n","                  next_jump -= 1\n","                  path.insert(0, prev_tag)\n","                  continue\n","              #Switch to next tag when we hit zero\n","              else:\n","                  segment.insert(0, lens[b]- j-1)\n","                  path.insert(0, next_tag)\n","              \n","              #Get the next tag, and the number of times we have to append the previous one\n","              prev_tag = next_tag\n","              max_tuple = argmax[next_tag]\n","              next_tag = max_tuple[0]\n","              next_jump = max_tuple[1]\n","              \n","          segments.append(segment)     \n","          paths.append(path)\n","          \n","      return paths, segments\n","\n","\n","  def _bilstm_score(self, logits, labels, seg_inds, lens):\n","      \n","      \"\"\"\n","      Computes the (batch_size,) numerator (FloatTensor list) for the log-likelihood, which is the\n","      \n","      Arguments:\n","          logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n","          labels: [batch_size, seq_len] LongTensor\n","          seg_inds: [batch_size, seq_len] LongTensor\n","          lens: [batch_size] LongTensor\n","      \"\"\"\n","      lens = Variable( lens, volatile = not self.training)\n","      \n","      batch_size, max_len, _, _ = logits.size()\n","      \n","      # Transpose to batch size and time dimensions\n","      labels = labels.transpose(1,0)\n","      \n","      seg_inds = seg_inds.transpose(1,0).data.cpu().numpy()\n","      labels_exp = labels.unsqueeze(-1)\n","\n","      #Construct the mask the will sellect the corrects segments from all possible segments for each timstep\n","      mask_seg = np.zeros(( batch_size, max_len, self.max_path))\n","      \n","      mask_step =  np.zeros(( batch_size), dtype=np.int32)\n","      counter = np.zeros((batch_size), dtype=np.int32)\n","      \n","      #For each timstep accross all sentences\n","      for i in range(0,max_len):\n","          #0 or 1 depending if we are on the end of a segment\n","          mask_step =  seg_inds[:, i] \n","          mask_seg[np.arange(batch_size), i, counter] = mask_step \n","          counter = counter + 1\n","          counter = (1- mask_step)*counter*(counter < self.max_path)\n","          \n","      mask_seg = torch.from_numpy(mask_seg).float()\n","      if next(self.parameters()).is_cuda == True:\n","          mask_seg = mask_seg.cuda()\n","          \n","      mask_seg = mask_seg.unsqueeze(-1).expand_as(logits)\n","      mask_seg = Variable(mask_seg,  volatile = not self.training) \n","      \n","      logit_mask = logits*mask_seg\n","      sum_cols = torch.sum(logit_mask, dim=2).squeeze(2)\n","      \n","      all_scores = torch.gather(sum_cols, 2, labels_exp).squeeze(-1)\n","      \n","      mask_time = self.sequence_mask(lens).float()\n","      all_scores = all_scores*mask_time\n","      \n","      sum_seg_scores = torch.sum(all_scores, dim=1).squeeze(-1)\n","\n","      return  sum_seg_scores\n","\n","  def score(self, logits, y, seg_inds, lens):\n","\n","\n","      bilstm_score = self._bilstm_score(logits, y, seg_inds, lens)\n","      transition_score = self.transition_score(y, lens, seg_inds )\n","      \n","      score = transition_score + bilstm_score\n","\n","      return score\n","  \n","  def transition_score(self, labels, lens, mask_seg_idx):\n","      \"\"\"\n","      Computes the (batch_size,) scores (FloatTensor list) that will be added to the emission scores\n","      \n","      Arguments:\n","          logits: [batch_size, seq_len, max_path, n_labels] FloatTensor\n","          labels: [batch_size, seq_len] LongTensor\n","          seg_inds: [batch_size, seq_len] LongTensor\n","          lens: [batch_size] LongTensor\n","      \"\"\"\n","      lens = Variable( lens, volatile = not self.training)\n","      labels = labels.transpose(1,0)\n","      mask_seg_idx = mask_seg_idx.transpose(1,0)\n","      batch_size, seq_len = labels.size()\n","      # pad labels with <start> and <stop> indices\n","      labels_ext = Variable(labels.data.new(batch_size, seq_len + 2))\n","      labels_ext[:, 0] = self.tag_to_ix['START']\n","      labels_ext[:, 1:-1] = labels\n","      mask = self.sequence_mask(lens + 1, max_len=seq_len + 2).long()\n","      pad_stop = Variable(labels.data.new(1).fill_(self.tag_to_ix['STOP']))\n","      \n","      pad_stop = pad_stop.unsqueeze(-1).expand(batch_size, seq_len + 2)\n","      labels_ext = (1 + (-1)*mask) * pad_stop + mask * labels_ext\n","      trn = self.transitions\n","      \n","      trn_exp = trn.unsqueeze(0).expand(batch_size, *trn.size())\n","      lbl_r = labels_ext[:, 1:]\n","      lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), trn.size(0))\n","      trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n","      \n","      lbl_lexp = labels_ext[:, :-1].unsqueeze(-1)\n","      trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n","      trn_scr = trn_scr.squeeze(-1)\n","      \n","      # Mask sentences in time dim\n","      mask = self.sequence_mask(lens + 1).float()\n","      trn_scr = trn_scr * mask\n","      \n","      trn_scr[:, 1:] = trn_scr[:, 1:].clone()*mask_seg_idx.float() \n","      \n","      score = trn_scr.sum(1).squeeze(-1)\n","      \n","      return score\n","\n","  def loglik(self, logits, y, lens):\n","      norm_score = self._forward_alg(logits, lens)\n","      sequence_score = self.score(logits, y, lens, logits=logits)\n","      loglik = sequence_score - norm_score\n","\n","      return loglik   \n","\n","\n","  def log_sum_exp(vec, dim=0, keepdim=True):\n","      max_val, idx = torch.max(vec, dim, keepdim=True)\n","      max_exp = max_val.expand_as(vec)\n","      \n","      return max_val + torch.log(torch.sum(torch.exp(vec - max_exp), dim, keepdim=keepdim))\n","\n","      \n","  def sequence_mask(lens, max_len=None):\n","      batch_size = lens.size(0)\n","      if max_len is None:\n","          \n","          max_len = lens.max().data[0]\n","              \n","      ranges = torch.arange(0, max_len).long()\n","      ranges = ranges.unsqueeze(0).expand(batch_size, max_len)\n","      ranges = Variable(ranges)\n","      if lens.data.is_cuda:\n","          ranges = ranges.cuda()\n","\n","      lens_exp = lens.unsqueeze(1).expand_as(ranges)\n","      mask = ranges < lens_exp\n","\n","      return mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jm_cNB4Aey5r"},"outputs":[],"source":["sentences = ['i like fresh bread', 'i hate stale bread', 'this bread is fresh']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1655220418811,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"eF5Rfr030Sfo","outputId":"aaa3e6c4-d596-490f-ff82-1b96a4d52abc"},"outputs":[],"source":["vocabulary = set()\n","for sent in sentences:\n","    for word in sent.split():\n","        vocabulary.add(word)\n","\n","print(vocabulary)\n","\n","word2index = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n","\n","for word in vocabulary:\n","    word2index[word] = len(word2index)\n","    \n","print(word2index)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1655220454736,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"nzuduC930zUZ","outputId":"252f18d4-cd6d-46e0-abf3-26d15e3c1803"},"outputs":[],"source":["batch = [ [ word2index[word] for word in sent.split() ] for sent in sentences]\n","\n","print(batch)\n","print()\n","\n","# Let's make a numpy array out of it\n","batch = np.array(batch)\n","\n","print(batch)\n","print('The shape of batch is:', batch.shape)\n","print()\n","\n","# Let's make a PyTorch tensor out of it\n","batch = torch.tensor(batch, dtype=torch.long)\n","print(batch)\n","print('The shape of batch is:', batch.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1655220526208,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"EvMbjCwx08F7","outputId":"b710fa6b-cf77-4fff-a9b7-2cd5dd43c9ae"},"outputs":[],"source":["#Word embeddings\n","\n","vocab_size = len(word2index)   # vocab_size reflects all known words in the index\n","embed_dim = 5                # Let's assume the word embeddings are of size 10 to keep it simple\n","\n","word_embedding_layer = nn.Embedding(vocab_size, embed_dim)\n","\n","batch = word_embedding_layer(batch)\n","\n","print('The shape of batch is:', batch.shape)\n","print()\n","print(batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eY_IbDlw1Njh"},"outputs":[],"source":["hidden_size = 32 # Let's set the size if the hidden dimension to 32\n","\n","lstm  = nn.LSTM(embed_dim, hidden_size, batch_first=True, bidirectional = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1655220710927,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"JTxbL_gL1h7n","outputId":"236765c9-cb35-48af-f080-903258ac8e01"},"outputs":[],"source":["# Initialize hidden state (here with zeros)\n","batch_size = batch.shape[0]\n","(h, c) =  (torch.zeros(2, batch_size, hidden_size), torch.zeros(2, batch_size, hidden_size)) \n","\n","lstm_out, (h, c) = lstm(batch, (h, c))\n","\n","print('The shape of lstm_out is:', lstm_out.shape) # (batch_size, seq_len, hidden_dim)\n","print('The shape of h is:', h.shape) # (num_layers*num_directions, batch_size, hidden_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Arj79Fh1zeG"},"outputs":[],"source":["forward_state = lstm_out[:, :, :hidden_size] \n","backward_state = lstm_out[:, :, hidden_size:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_ICCafi4Djf"},"outputs":[],"source":["batch_size =  len(sentences)\n","sen_len = lstm_out.shape[1]\n","all_spans = []\n","span_indices = []\n","for i in range(batch_size):\n","  for j in range(i, batch_size):\n","    span_indices.append((i, j))\n","    if i == 0:\n","      forward_previous = np.zeros((sen_len, hidden_size))\n","    else:\n","      forward_previous = lstm_out[i-1, :, :hidden_size].cpu().detach().numpy()\n","    backward_current = lstm_out[i, :, hidden_size:]\n","    forward_j = lstm_out[j, :, :hidden_size]\n","    if j == (batch_size - 1):\n","      backward_next = np.zeros((sen_len, hidden_size))\n","    else:\n","      backward_next = lstm_out[j+1, :, hidden_size:].cpu().detach().numpy()\n","    span = np.concatenate((forward_previous, backward_current.cpu().detach().numpy(), forward_j.cpu().detach().numpy(), backward_next), axis=0)\n","    all_spans.append(span)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1655225471204,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"aJ3UzfxA_LjQ","outputId":"407b8de2-9879-4299-b348-f5c2e6710317"},"outputs":[],"source":["span_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388,"status":"ok","timestamp":1655224798411,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"lH10cqBT_Lxu","outputId":"2199380f-3731-447b-f027-ba128b3392c0"},"outputs":[],"source":["lstm_out[2, :, :hidden_size].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655226025035,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"HAj68q5VFaCj","outputId":"3455fe84-3143-44eb-8bc2-b673cb067209"},"outputs":[],"source":["all_spans_tensor = torch.tensor(all_spans, dtype=torch.long)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655226051461,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"w9crTKJ3I9wE","outputId":"dad1d142-1f94-4943-b0f4-aec2967702d9"},"outputs":[],"source":["all_spans_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":549,"status":"ok","timestamp":1655226163613,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"pfip3uVUKSfN","outputId":"ffadec88-1d07-49d5-ceac-030fbbad5bdd"},"outputs":[],"source":["all_spans[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlhkIDOXKt3N"},"outputs":[],"source":["from allennlp.common.util import pad_sequence_to_length\n","from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n","from allennlp.nn.util import masked_mean, masked_softmax\n","import copy\n","\n","from transformers import BertModel\n","\n","from allennlp.modules import ConditionalRandomField\n","\n","import torch\n","\n","\n","class CRFOutputLayer(torch.nn.Module):\n","    ''' CRF output layer consisting of a linear layer and a CRF. '''\n","    def __init__(self, in_dim, num_labels):\n","        super(CRFOutputLayer, self).__init__()\n","        self.num_labels = num_labels\n","        self.classifier = torch.nn.Linear(in_dim, self.num_labels)\n","        self.crf = ConditionalRandomField(self.num_labels)\n","\n","    def forward(self, x, mask, labels=None):\n","        ''' x: shape: batch, max_sequence, in_dim\n","            mask: shape: batch, max_sequence\n","            labels: shape: batch, max_sequence\n","        '''\n","\n","        batch_size, max_sequence, in_dim = x.shape\n","\n","        logits = self.classifier(x)\n","        outputs = {}\n","        if labels is not None:\n","            log_likelihood = self.crf(logits, labels, mask)\n","            loss = -log_likelihood\n","            outputs[\"loss\"] = loss\n","        else:\n","            best_paths = self.crf.viterbi_tags(logits, mask)\n","            predicted_label = [x for x, y in best_paths]\n","            predicted_label = [pad_sequence_to_length(x, desired_length=max_sequence) for x in predicted_label]\n","            predicted_label = torch.tensor(predicted_label)\n","            outputs[\"predicted_label\"] = predicted_label\n","\n","            #log_denominator = self.crf._input_likelihood(logits, mask)\n","            #log_numerator = self.crf._joint_likelihood(logits, predicted_label, mask)\n","            #log_likelihood = log_numerator - log_denominator\n","            #outputs[\"log_likelihood\"] = log_likelihood\n","\n","        return outputs\n","\n","\n","class AttentionPooling(torch.nn.Module):\n","    def __init__(self, in_features, dimension_context_vector_u=200, number_context_vectors=5):\n","        super(AttentionPooling, self).__init__()\n","        self.dimension_context_vector_u = dimension_context_vector_u\n","        self.number_context_vectors = number_context_vectors\n","        self.linear1 = torch.nn.Linear(in_features=in_features, out_features=self.dimension_context_vector_u, bias=True)\n","        self.linear2 = torch.nn.Linear(in_features=self.dimension_context_vector_u,\n","                                       out_features=self.number_context_vectors, bias=False)\n","\n","        self.output_dim = self.number_context_vectors * in_features\n","\n","    def forward(self, tokens, mask):\n","        #shape tokens: (batch_size, tokens, in_features)\n","\n","        # compute the weights\n","        # shape tokens: (batch_size, tokens, dimension_context_vector_u)\n","        a = self.linear1(tokens)\n","        a = torch.tanh(a)\n","        # shape (batch_size, tokens, number_context_vectors)\n","        a = self.linear2(a)\n","        # shape (batch_size, number_context_vectors, tokens)\n","        a = a.transpose(1, 2)\n","        a = masked_softmax(a, mask)\n","\n","        # calculate weighted sum\n","        s = torch.bmm(a, tokens)\n","        s = s.view(tokens.shape[0], -1)\n","        return s\n","\n","\n","\n","class BertTokenEmbedder(torch.nn.Module):\n","    def __init__(self, config):\n","        super(BertTokenEmbedder, self).__init__()\n","        self.bert = BertModel.from_pretrained(config[\"bert_model\"])\n","        # state_dict_1 = self.bert.state_dict()\n","        # state_dict_2 = torch.load('/home/astha_agarwal/model/pytorch_model.bin')\n","        # for name2 in state_dict_2.keys():\n","        #    for name1 in state_dict_1.keys():\n","        #        temp_name = copy.deepcopy(name2)\n","        #       if temp_name.replace(\"bert.\", '') == name1:\n","        #            state_dict_1[name1] = state_dict_2[name2]\n","\n","        #self.bert.load_state_dict(state_dict_1,strict=False)\n","\n","        self.bert_trainable = config[\"bert_trainable\"]\n","        self.bert_hidden_size = self.bert.config.hidden_size\n","        self.cacheable_tasks = config[\"cacheable_tasks\"]\n","        for param in self.bert.parameters():\n","            param.requires_grad = self.bert_trainable\n","\n","    def forward(self, batch):\n","        documents, sentences, tokens = batch[\"input_ids\"].shape\n","\n","        if \"bert_embeddings\" in batch:\n","            return batch[\"bert_embeddings\"]\n","\n","        attention_mask = batch[\"attention_mask\"].view(-1, tokens)\n","        input_ids = batch[\"input_ids\"].view(-1, tokens)\n","\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        # shape (documents*sentences, tokens, 768)\n","        bert_embeddings = outputs[0]\n","\n","        if not self.bert_trainable and batch[\"task\"] in self.cacheable_tasks:\n","            # cache the embeddings of BERT if it is not fine-tuned\n","            # to save GPU memory put the values on CPU\n","            batch[\"bert_embeddings\"] = bert_embeddings.to(\"cpu\")\n","\n","        return bert_embeddings\n","\n","class BertHSLN(torch.nn.Module):\n","    '''\n","    Model for Baseline, Sequential Transfer Learning and Multitask-Learning with all layers shared (except output layer).\n","    '''\n","    def __init__(self, config, num_labels):\n","        super(BertHSLN, self).__init__()\n","\n","        self.bert = BertTokenEmbedder(config)\n","\n","        # Jin et al. uses DROPOUT WITH EXPECTATION-LINEAR REGULARIZATION (see Ma et al. 2016),\n","        # we use instead default dropout\n","        self.dropout = torch.nn.Dropout(config[\"dropout\"])\n","\n","        self.generic_output_layer = config.get(\"generic_output_layer\")\n","\n","        self.lstm_hidden_size = config[\"word_lstm_hs\"]\n","\n","        self.word_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(input_size=self.bert.bert_hidden_size,\n","                                  hidden_size=self.lstm_hidden_size,\n","                                  num_layers=1, batch_first=True, bidirectional=True))\n","\n","        self.attention_pooling = AttentionPooling(2 * self.lstm_hidden_size,\n","                                                  dimension_context_vector_u=config[\"att_pooling_dim_ctx\"],\n","                                                  number_context_vectors=config[\"att_pooling_num_ctx\"])\n","\n","        self.sentence_lstm_hidden_size = config[\"sentence_lstm_hs\"]\n","        input_dim = self.attention_pooling.output_dim\n","        print(f\"Attention pooling dim: {input_dim}\")\n","        self.sentence_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(input_size=input_dim,\n","                                  hidden_size=self.sentence_lstm_hidden_size,\n","                                  num_layers=1, batch_first=True, bidirectional=True))\n","\n","        #self.reinit_output_layer(tasks, config)\n","        self.sentence_lstm_hidden_size = config[\"sentence_lstm_hs\"]\n","\n","        self.endpoint_span_extractor = EndpointSpanExtractor(self.sentence_lstm_hidden_size * 2, \n","                                                             combination = \"x,y,x*y,x-y\", \n","                                                             num_width_embeddings = config[\"max_path\"], \n","                                                             span_width_embedding_dim = config[\"span_width_embedding_dim\"], \n","                                                             bucket_widths = True)\n","        \n","        self.input_dim  = self.sentence_lstm_hidden_size * 2\n","        self.max_path = config[\"max_path\"]\n","        self.num_labels =  num_labels\n","        \n","        \n","        self._span_crf = config[\"span_crf\"]\n","        self._crf = config[\"crf\"]\n","\n","        if self._crf:\n","          self.crf_fc  = nn.Linear(self.input_dim, num_labels)\n","          self.crf = CRFOutputLayer(in_dim  = self.input_dim, num_labels = num_labels)\n","\n","        if self._span_crf:\n","          self.span_input_dim = self.sentence_lstm_hidden_size * 2 * 4 * config[\"span_width_embedding_dim\"]\n","          self.crf_spanfc = nn.Linear(self.span_input_dim, self.num_labels)\n","          self.spancrf = SpanCRF(config[\"label_to_ind\"], self.max_path)\n","          self.b = batch[\"label_ids\"]\n","          \n","\n","\n","\n","    def init_sentence_enriching(self, config, tasks):\n","        self.sentence_lstm_hidden_size = config[\"sentence_lstm_hs\"]\n","        input_dim = self.attention_pooling.output_dim\n","        print(f\"Attention pooling dim: {input_dim}\")\n","        self.sentence_lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(input_size=input_dim,\n","                                  hidden_size=self.sentence_lstm_hidden_size,\n","                                  num_layers=1, batch_first=True, bidirectional=True))\n","\n","    def reinit_output_layer(self, tasks, config):\n","        if config.get(\"without_context_enriching_transfer\"):\n","            self.init_sentence_enriching(config, tasks)\n","        input_dim = self.lstm_hidden_size * 2\n","\n","        if self.generic_output_layer:\n","            self.crf = CRFOutputLayer(in_dim=input_dim, num_labels=len(tasks[0].labels))\n","        else:\n","            self.crf = CRFPerTaskOutputLayer(input_dim, tasks)\n","\n","    def forward(self, batch, labels=None, output_all_tasks=False):\n","\n","        documents, sentences, tokens = batch[\"input_ids\"].shape\n","\n","        # shape (documents*sentences, tokens, 768)\n","        bert_embeddings = self.bert(batch)\n","\n","        # in Jin et al. only here dropout\n","        bert_embeddings = self.dropout(bert_embeddings)\n","\n","        tokens_mask = batch[\"attention_mask\"].view(-1, tokens)\n","        # shape (documents*sentences, tokens, 2*lstm_hidden_size)\n","        bert_embeddings_encoded = self.word_lstm(bert_embeddings, tokens_mask)\n","\n","\n","        # shape (documents*sentences, pooling_out)\n","        # sentence_embeddings = torch.mean(bert_embeddings_encoded, dim=1)\n","        sentence_embeddings = self.attention_pooling(bert_embeddings_encoded, tokens_mask)\n","        # shape: (documents, sentences, pooling_out)\n","        sentence_embeddings = sentence_embeddings.view(documents, sentences, -1)\n","        # in Jin et al. only here dropout\n","        sentence_embeddings = self.dropout(sentence_embeddings)\n","\n","\n","        sentence_mask = batch[\"sentence_mask\"]\n","\n","        # shape: (documents, sentence, 2*lstm_hidden_size)\n","        sentence_embeddings_encoded = self.sentence_lstm(sentence_embeddings, sentence_mask)\n","        # in Jin et al. only here dropout\n","        sentence_embeddings_encoded = self.dropout(sentence_embeddings_encoded)\n","\n","        sentence_len = torch.sum(sentence_mask, dim  = 1)\n","        output  = {}\n","\n","        if self.span_crf:\n","          span_embeddings =  self.endpoint_span_extractor(sentence_embeddings_encoded, batch[\"span_indices\"], sentence_mask)\n","          segment_rep  = self.crf_spanfc(span_embeddings)\n","          _, max_span_len, _ = segment_rep.shape\n","\n","          segment_span_feat  = torch.zeros(documents, sentences, self.max_path, self.num_labels)\n","\n","          batch_size, max_span_len, _ = batch[\"span_indices\"].shape\n","          _, max_seq_len, max_path_len, _ = segment_span_feat.shape\n","\n","          for i in range(batch_size):\n","            for j in range(max_span_len):\n","              start_idx  = batch[\"span_indices\"][i][j][0]\n","              len_idx  = batch[\"span_indices\"][i][j][1] - start_idx\n","              segment_span_feat[i, start_idx, len_idx, :] = segment_rep[i][j]\n","\n","          segment_mask = batch[\"segment_mask\"]\n","\n","          span_forward_var_batch = self.spancrf._forward_alg(segment_span_feat, sentence_len)\n","          span_gold_score_batch = self.spancrf.score(segment_span_feat, labels.transpose(0,1), segment_mask.transpose(0,1), sentence_len)\n","          output['span_crf'] = {\"forwrd_var_batch\": span_forward_var_batch, \"gold_score_batch\": span_gold_score_batch}\n","\n","        if self.crf:\n","          segment_feat  = sentence_embeddings_encoded.unsqueeze(2)\n","          segment_feat = self.crf_fc(segment_feat)\n","          segment_feat  = segment_feat.view(documents, snetencces, 1, self.num_labels)\n","\n","          forward_var_batch = self.crf._forward_alg(segment_feat, sentence_len)\n","          gold_score_batch = self.crf.score(segment_feat, labels.transpose(0,1), sentence_mask.transpose(0,1), sentence_len)\n","          output['crf'] = {\"forwrd_var_batch\": forward_var_batch, \"gold_score_batch\": gold_score_batch}\n","\n","        if eval:\n","          if self.crf:\n","            crf_tag_seqs, crf_segments = self.crf.viterbi_decode(segment_feat, sentence_len)\n","            output['crf'] = {\"tag_seqs\": crf_tag_seqs, \"segments\": crf_segments}\n","          if self.span_crf:\n","            span_crf_tag_seqs, span_crf_segments = self.spancrf.viterbi_decode(segment_span_feat, sentence_len)\n","            output['span_crf'] = {\"tag_seqs\": span_crf_tag_seqs, \"segments\": span_crf_segments}\n","\n","\n","        # if self.generic_output_layer:\n","        #     output = self.crf(sentence_embeddings_encoded, sentence_mask, labels)\n","        # else:\n","        #     output = self.crf(batch[\"task\"], sentence_embeddings_encoded, sentence_mask, labels, output_all_tasks)\n","\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1PVTPFt9REH"},"outputs":[],"source":["!pip install allennlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQpQVsrc9oom"},"outputs":[],"source":["!pip install --upgrade google-cloud-storage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NaNOx26v9omf"},"outputs":[],"source":["from allennlp.data.dataset_readers.dataset_utils import enumerate_spans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmorHJsw-W6X"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgX-PfJuNTao"},"outputs":[],"source":["def get_span_indices(sentence_mask):\n","  all_span_ids = []\n","  for each in range(len(sentence_mask)):\n","    each_span_ids = enumerate_spans(sentence_mask[each][(sentence_mask[each].nonzero())])\n","    print(\"Each \", each_span_ids)\n","    all_span_ids.append(each_span_ids)\n","\n","  max_span_len = max(len(x) for x in all_span_ids)\n","  span_ids = [x+[[0,0]]*(max_span_len-len(x)) for x in all_span_ids]\n","  span_indices = torch.tensor(span_ids)\n","  return span_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655744147556,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"aoMfAumWNTf7","outputId":"1dffa4aa-4cab-4938-8764-d3b1f10d761b"},"outputs":[],"source":["s_mask = torch.tensor([[1,1,1,1],[1,0,0,0]])\n","s_indices = get_span_indices(s_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1655744151223,"user":{"displayName":"Irtiza Chowdhury ,144430","userId":"06332862582352150667"},"user_tz":-120},"id":"sUZIS8BsNTii","outputId":"9dd25b8e-5df6-420c-ea8a-ec713078a334"},"outputs":[],"source":["s_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af5EcsZ_NTlA"},"outputs":[],"source":["def get_segment_mask():\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xzx4HOJtNTnm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL_-wwPzNTqy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"La5_8eR7NTso"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSatgOI9NTvd"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP723DGl48OpTdzMRcEIB+A","collapsed_sections":[],"name":"SpanCRF.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
